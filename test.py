import json
import os
from datetime import datetime
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

MODEL_NAME = "llama3"
model = OllamaLLM(
    model=MODEL_NAME
)

question_template = """
‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

{context}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢" ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 1 ‡∏Ç‡πâ‡∏≠
‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ô‡∏¥‡∏™‡∏±‡∏¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
"""

summary_template = """
‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ:

{context}

‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢ ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå ‡πÅ‡∏•‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 5 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
"""

question_prompt = ChatPromptTemplate.from_template(question_template)
summary_prompt = ChatPromptTemplate.from_template(summary_template)

question_chain = question_prompt | model
summary_chain = summary_prompt | model

conversation_log = {
    "questions": [],
    "answers": [],
    "ollama_generated_questions": [],
    "summary": "",
    "all_answers": [],  # ‚úÖ ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÉ‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏î‡∏µ‡∏¢‡∏ß
    "model_thoughts": {
        "question_generation": [],
        "summary_generation": {}
    }
}

def handle_conversation(num_questions=5):
    context = ""

    print("üí¨ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏à‡∏£‡∏¥‡∏á ‡∏û‡∏¥‡∏°‡∏û‡πå 'exit' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î\n")

    for i in range(num_questions):
        if i == 0:
            user_question = "‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ñ‡∏∏‡∏ì‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤‡∏≠‡∏∞‡πÑ‡∏£?"
        else:
            prompt_text = question_template.replace("{context}", context)
            user_question = model.invoke(prompt_text).strip()

            conversation_log["model_thoughts"]["question_generation"].append({
                "prompt": prompt_text,
                "response": user_question
            })
            conversation_log["ollama_generated_questions"].append(user_question)

        print(f"Q{i+1}: {user_question}")
        user_input = input("You: ").strip()

        if user_input.lower() == "exit":
            print("üëã ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡πâ‡∏ß")
            break

        # ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏•‡∏á context ‡πÅ‡∏•‡∏∞ log
        context += f"\n‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {user_input}"
        conversation_log["questions"].append(user_question)
        conversation_log["answers"].append(user_input)

        # ‚úÖ ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡∏û‡∏£‡πâ‡∏≠‡∏° timestamp ‡πÑ‡∏ß‡πâ‡πÉ‡∏ô all_answers
        conversation_log["all_answers"].append({
            "timestamp": datetime.now().isoformat(),
            "answer": user_input
        })

    print("\nüß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ...\n")

    summary_prompt_text = summary_template.replace("{context}", context)
    summary = model.invoke(summary_prompt_text).strip()

    conversation_log["model_thoughts"]["summary_generation"] = {
        "prompt": summary_prompt_text,
        "response": summary
    }

    conversation_log["summary"] = summary

    print("üß† ‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å/‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:\n")
    print(summary)

    save_conversation_to_json()

def save_conversation_to_json(filename="conversation_log.json"):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(conversation_log, f, ensure_ascii=False, indent=2)
    print(f"\nüìÅ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå: {filename}")

if __name__ == "__main__":
    handle_conversation()
