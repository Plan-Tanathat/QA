import json
import os
from langchain_ollama import OllamaLLM
from langchain_core.prompts import ChatPromptTemplate

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•
MODEL_NAME = "llama3"

# ‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
model = OllamaLLM(model=MODEL_NAME)

# Template ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏° (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
question_template = """
‡∏à‡∏≤‡∏Å‡∏õ‡∏£‡∏∞‡∏ß‡∏±‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

{context}

‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏™‡∏£‡πâ‡∏≤‡∏á "‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÉ‡∏´‡∏°‡πà‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢" ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô 1 ‡∏Ç‡πâ‡∏≠
‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ô‡∏¥‡∏™‡∏±‡∏¢ ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å ‡∏´‡∏£‡∏∑‡∏≠‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö ‡∏ï‡∏£‡∏á‡∏õ‡∏£‡∏∞‡πÄ‡∏î‡πá‡∏ô ‡πÅ‡∏•‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 1 ‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ
"""

# Template ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå (‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢)
summary_template = """
‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ô‡∏µ‡πâ:

{context}

‡∏ä‡πà‡∏ß‡∏¢‡∏™‡∏£‡∏∏‡∏õ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏ô‡∏¥‡∏™‡∏±‡∏¢ ‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå ‡πÅ‡∏•‡∏∞‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ
‡∏™‡∏£‡∏∏‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏†‡∏≤‡∏©‡∏≤‡πÑ‡∏ó‡∏¢ ‡∏Å‡∏£‡∏∞‡∏ä‡∏±‡∏ö‡πÑ‡∏°‡πà‡πÄ‡∏Å‡∏¥‡∏ô 5 ‡∏ö‡∏£‡∏£‡∏ó‡∏±‡∏î
"""

# ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° prompt templates
question_prompt = ChatPromptTemplate.from_template(question_template)
summary_prompt = ChatPromptTemplate.from_template(summary_template)

question_chain = question_prompt | model
summary_chain = summary_prompt | model

# ‡πÄ‡∏Å‡πá‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î
conversation_log = {
    "questions": [],
    "answers": [],
    "ollama_generated_questions": [],
    "summary": "",
    "model_thoughts": {
        "question_generation": [],
        "summary_generation": {}
    }
}

def load_previous_conversation(filename="conversation_log.json"):
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            data = json.load(f)
            return data
    return None

def handle_conversation(num_questions=5):
    old_data = load_previous_conversation()
    context = ""

    if old_data:
        print("üóÇÔ∏è ‡πÄ‡∏à‡∏≠‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÄ‡∏Å‡πà‡∏≤ ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ô‡∏≥‡∏°‡∏≤‡πÉ‡∏ä‡πâ‡∏ï‡πà‡∏≠‡∏¢‡∏≠‡∏î...")
        for q, a in zip(old_data.get("questions", []), old_data.get("answers", [])):
            context += f"\n‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {a}"
        conversation_log["questions"].extend(old_data.get("questions", []))
        conversation_log["answers"].extend(old_data.get("answers", []))
        conversation_log["ollama_generated_questions"].extend(old_data.get("ollama_generated_questions", []))
        conversation_log["model_thoughts"]["question_generation"].extend(old_data.get("model_thoughts", {}).get("question_generation", []))

    print("üí¨ ‡πÄ‡∏£‡∏¥‡πà‡∏°‡∏ï‡πâ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÉ‡∏´‡∏°‡πà! ‡∏Å‡∏£‡∏∏‡∏ì‡∏≤‡∏ï‡∏≠‡∏ö‡∏ï‡∏≤‡∏°‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡∏π‡πâ‡∏™‡∏∂‡∏Å‡∏à‡∏£‡∏¥‡∏á ‡∏û‡∏¥‡∏°‡∏û‡πå 'exit' ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏¢‡∏∏‡∏î\n")

    for i in range(num_questions):
        if i == 0 and not context:
            user_question = "‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏´‡∏ô‡∏∂‡πà‡∏á‡∏Ñ‡∏≥‡∏ó‡∏µ‡πà‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ñ‡∏∏‡∏ì‡∏ß‡∏±‡∏ô‡∏ô‡∏µ‡πâ ‡∏à‡∏∞‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡∏Ñ‡∏≥‡∏ß‡πà‡∏≤‡∏≠‡∏∞‡πÑ‡∏£?"
        else:
            # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° prompt ‡∏ó‡∏µ‡πà‡∏™‡πà‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ
            prompt_text = question_template.replace("{context}", context)
            user_question = model.invoke(prompt_text).strip()

            # ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà Model ‡∏Ñ‡∏¥‡∏î‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö
            conversation_log["model_thoughts"]["question_generation"].append({
                "prompt": prompt_text,
                "response": user_question
            })
            conversation_log["ollama_generated_questions"].append(user_question)

        print(f"Q{i+1}: {user_question}")
        user_input = input("You: ").strip()
        
        if user_input.lower() == "exit":
            print("üëã ‡∏à‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏ô‡∏ó‡∏ô‡∏≤‡πÅ‡∏•‡πâ‡∏ß")
            break

        context += f"\n‡∏ú‡∏π‡πâ‡πÉ‡∏ä‡πâ: {user_input}"
        conversation_log["questions"].append(user_question)
        conversation_log["answers"].append(user_input)

    print("\nüß† ‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡∏ó‡∏µ‡πà‡∏°‡∏µ...\n")

    # ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° prompt ‡∏™‡∏£‡∏∏‡∏õ
    summary_prompt_text = summary_template.replace("{context}", context)
    summary = model.invoke(summary_prompt_text).strip()

    # ‡πÄ‡∏Å‡πá‡∏ö‡∏™‡∏¥‡πà‡∏á‡∏ó‡∏µ‡πà Model ‡∏Ñ‡∏¥‡∏î‡∏ï‡∏≠‡∏ô‡∏™‡∏£‡∏∏‡∏õ
    conversation_log["model_thoughts"]["summary_generation"] = {
        "prompt": summary_prompt_text,
        "response": summary
    }

    conversation_log["summary"] = summary

    print("üß† ‡∏ú‡∏•‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏∏‡∏Ñ‡∏•‡∏¥‡∏Å/‡∏≠‡∏≤‡∏£‡∏°‡∏ì‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏∏‡∏ì:\n")
    print(summary)

    save_conversation_to_json()

def save_conversation_to_json(filename="conversation_log.json"):
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(conversation_log, f, ensure_ascii=False, indent=2)
    print(f"\nüìÅ ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î‡πÑ‡∏ß‡πâ‡∏ó‡∏µ‡πà‡πÑ‡∏ü‡∏•‡πå: {filename}")

if __name__ == "__main__":
    handle_conversation()
